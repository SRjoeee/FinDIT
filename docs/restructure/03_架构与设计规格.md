# FindIt 架构与设计规格

> **版本**: v1.1
> **日期**: 2026-02-11
> **适用范围**: 重构后的目标架构、协议定义、数据库 Schema、数据流、性能指标
> **前置文档**: [01\_竞品分析报告](./01_竞品分析报告.md)、[02\_技术决策记录](./02_技术决策记录.md)

---

## 目录

- [第一部分：架构总览](#第一部分架构总览)
- [第二部分：MediaService 媒体抽象层](#第二部分mediaservice-媒体抽象层)
- [第三部分：CLIP 嵌入与向量索引层](#第三部分clip-嵌入与向量索引层)
- [第四部分：分层索引管线](#第四部分分层索引管线)
- [第五部分：查询管线与搜索引擎](#第五部分查询管线与搜索引擎)
- [第六部分：数据库 Schema 变更](#第六部分数据库-schema-变更)
- [第七部分：数据流](#第七部分数据流)
- [第八部分：性能指标与基准](#第八部分性能指标与基准)

---

# 第一部分：架构总览

## 1.1 现有架构

当前的 FindIt 已完成 Stage 0–4c，整体结构如下：

```
┌─────────────────────────────────────────────┐
│           macOS App (SwiftUI)               │  ← Stage 4
│          搜索框 · 网格 · 侧边栏 · 预览       │
├─────────────────────────────────────────────┤
│           FindItCLI (验证层)                 │  ← Stage 0-3
│          子命令入口 · 结果格式化               │
├─────────────────────────────────────────────┤
│           FindItCore (核心库)                │
│                                             │
│   ┌────────────┐  ┌───────────────────┐    │
│   │SearchEngine │  │  PipelineManager  │    │
│   │ FTS5+向量   │  │  调度 STT/视觉管线 │    │
│   │ VectorStore │  │  IndexingScheduler│    │
│   └──────┬─────┘  └─────────┬─────────┘    │
│          │                   │               │
│   ┌──────┴─────┐  ┌─────────┴──────────┐   │
│   │  Database   │  │  FFmpegBridge      │   │
│   │  GRDB/FTS5  │  │  STTProcessor      │   │
│   │  向量 BLOB  │  │  VisionAnalyzer    │   │
│   └────────────┘  └────────────────────┘   │
├─────────────────────────────────────────────┤
│   双层 SQLite 存储                           │
│   文件夹级库: <素材文件夹>/.clip-index/       │
│   全局搜索索引: ~/Library/App Support/FindIt/ │
└─────────────────────────────────────────────┘
```

**核心管线（6 阶段顺序执行）**：

```
视频 → FFmpeg 场景检测 → 关键帧提取 → Apple Vision 本地分析
     → WhisperKit STT → Gemini/VLM 视觉描述 → 文本嵌入 → FTS5 + 向量搜索
```

**四个根本性瓶颈**（决策理由详见 [02\_技术决策记录](./02_技术决策记录.md)）：

| # | 问题 | 根因 |
|---|------|------|
| 1 | **信息瓶颈** — 搜索"红色跑车"依赖 VLM 描述恰好含这四个字 | 图像→文字→嵌入的间接路径丢失视觉信息 |
| 2 | **外部依赖** — Gemini API 延迟 2-5s/帧 + 隐私风险 | VLM 描述是管线最慢环节 |
| 3 | **格式局限** — 无法处理 BRAW/R3D/ARRIRAW 等专业格式 | FFmpeg 硬编码，无抽象层 |
| 4 | **语言受限** — 不支持中文视觉搜索 | 当前 CLIP 无中文训练，FTS5 依赖描述语言 |

## 1.2 目标架构

重构后的架构在保留顶层（SwiftUI App、双层 SQLite）的同时，替换中间处理引擎：

```
┌──────────────────────────────────────────────────────────────────┐
│                    macOS App (SwiftUI)                          │
│              搜索框 · 以图搜索 · 网格 · 侧边栏 · 预览            │
├──────────────────────────────────────────────────────────────────┤
│                    FindItApp 层                                 │
│   IndexingManager · SearchState · AppState · VolumeMonitor     │
├──────────────────────────────────────────────────────────────────┤
│                    FindItCore 层                                │
│                                                                 │
│  ┌──────────────┐  ┌─────────────────┐  ┌─────────────────┐   │
│  │ QueryPipeline│  │  LayeredIndexer │  │  MediaService   │   │
│  │ 查询预处理    │  │  分层索引调度    │  │  协议路由层     │   │
│  │ 翻译·扩展    │  │  L0→L1→L2→L3   │  │  探测·派发     │   │
│  └───────┬──────┘  └────────┬────────┘  └────────┬────────┘   │
│          │                  │                     │             │
│  ┌───────┴──────┐  ┌───────┴────────┐  ┌────────┴────────┐   │
│  │SearchEngine  │  │ CLIP Encoders  │  │  Decoders       │   │
│  │ FTS5+HNSW   │  │ Image/Text     │  │  AVFoundation   │   │
│  │ 融合排序     │  │ (CoreML/ONNX)  │  │  FFmpeg         │   │
│  │ 负向查询     │  │ LRU 缓存       │  │  BRAW SDK       │   │
│  │ VectorStore  │  └────────────────┘  │  (未来: R3D等)  │   │
│  └──────────────┘                      └─────────────────┘   │
│                                                                 │
│  ┌────────────────────────────────────────────────────────┐   │
│  │ 保持不变的模块                                          │   │
│  │ Database · SyncEngine · STTProcessor · VisionField     │   │
│  │ LocalVisionAnalyzer · WhisperKit · RateLimiter · ...  │   │
│  └────────────────────────────────────────────────────────┘   │
├──────────────────────────────────────────────────────────────────┤
│  双层 SQLite 存储 (不变)                                        │
│  文件夹级库: .clip-index/index.sqlite (source of truth)        │
│  全局搜索索引: ~/Library/App Support/FindIt/search.sqlite      │
│                                                                 │
│  USearch 双索引 (向量缓存, 可从 SQLite 重建):                     │
│  ~/Library/App Support/FindIt/clip.usearch  (CLIP space, L1)  │
│  ~/Library/App Support/FindIt/text.usearch  (text emb, L3)    │
└──────────────────────────────────────────────────────────────────┘
```

## 1.3 重构原则

1. **替换底层，保留顶层** — UI 层和存储层几乎不变，替换的是中间处理引擎
2. **协议先行** — 每个新模块先定义 protocol，再写实现，再写测试
3. **渐进替换** — 每个 Phase 独立可验证，旧路径保留为 fallback 直到新路径稳定
4. **测试驱动** — 新模块 100% 配备单元测试；重构模块保持现有测试全部通过
5. **零回归** — 每个 Phase 完成后 `swift test` 全部通过 (448+ 现有 + 新增)
6. **最小侵入** — 能不改的文件绝对不改；必须改的文件只改必须改的部分
7. **多媒体预埋** — 新增数据结构和协议在设计时预留照片/音频扩展点

## 1.4 影响范围概要

基于代码质量审计（详见 [02\_技术决策记录 §1.1](./02_技术决策记录.md)），项目整体代码质量 A 级。以下是文件处置分类：

| 分类 | 文件数 | 占比 | 说明 |
|------|--------|------|------|
| **完全不动** | ~33 | ~50% | App Views、Database、SyncEngine、Utils、CxxHash 等 |
| **需要修改** | ~12 | ~19% | PipelineManager、SceneDetector、KeyframeExtractor、AudioExtractor、FFmpegBridge、SearchEngine、VectorStore、Migrations 等 |
| **新增文件** | ~15 | ~11% | Media/ 目录全部、CLIP 编码器、LayeredIndexer、QueryPipeline、TranslationBridge 等 |

**重构比例**：约 25% 代码需要改动，约 75% 完全不动。

---

# 第二部分：MediaService 媒体抽象层

## 2.1 设计哲学

借鉴 GStreamer 和 DaVinci Resolve 的成功经验，采用**分层抽象 + 注册发现 + 优先级路由**的架构。

核心洞察：FindIt 不是 NLE（不需要实时播放和色彩管线），而是**素材资产管理工具**。核心需求是：
- **能读取** — 提取缩略图/关键帧（不需要完整 debayer 精度）
- **能索引** — 提取元数据（时长、分辨率、编码信息）
- **能搜索** — 生成语义向量

这大幅降低了格式支持的复杂度：不需要实时播放能力，不需要完整色彩管线（sRGB 缩略图即可），甚至可以接受低质量的预览帧（用于 CLIP 编码）。

## 2.2 协议层次结构

```
┌─────────────────────────────────────────────────┐
│              应用层 (App / Pipeline)              │
│     LayeredIndexer, SearchEngine, UI             │
│     ※ 完全不感知具体用了哪个解码器                 │
├─────────────────────────────────────────────────┤
│           MediaService (统一协议层)               │
│     probe · detectScenes · extractKeyframes      │
│     extractAudio · extractFrames · getDuration   │
├─────────────────────────────────────────────────┤
│         CompositeMediaService (路由层)            │
│   ┌─────────┐ ┌─────────┐ ┌─────────┐          │
│   │ Format  │→│ Decoder │→│ Fallback│          │
│   │ Detect  │ │ Select  │ │ Chain   │          │
│   └─────────┘ └─────────┘ └─────────┘          │
├──────┬──────┬──────┬──────┬──────┬──────────────┤
│ AVF  │BRAW  │ RED  │FFmpeg│Future│ Metadata-Only│
│Bridge│SDK   │ SDK  │Bridge│ SDK  │ (sidecar)    │
│P:80  │P:150 │P:150 │P:50  │P:150 │P:10          │
└──────┴──────┴──────┴──────┴──────┴──────────────┘

P = Priority（数字越高越优先）
```

## 2.3 核心类型定义

```swift
/// 媒体解码器能力声明
public struct MediaCapability: Sendable, Hashable {
    /// 支持的文件扩展名（小写，不含点号）
    public let fileExtensions: Set<String>
    /// 支持的 UTType 标识符
    public let utTypes: Set<String>
    /// 解码器名称（日志/调试用）
    public let name: String
    /// 优先级（数字越大越优先，相同优先级按注册顺序）
    public let priority: Int
}

/// 媒体类型枚举（预埋：未来支持照片和音频）
public enum MediaType: String, Codable, Sendable {
    case video
    case photo   // 未来扩展：jpg, png, heic, tiff, webp, raw, dng
    case audio   // 未来扩展：mp3, wav, aac, flac, m4a, aiff
}

/// 格式探测结果
public struct ProbeResult: Sendable {
    public let score: Int              // 0-100，0 表示不支持
    public let mediaType: MediaType
    public let containerFormat: String?
    public let codec: String?
    public let duration: Double?       // 秒，照片为 nil
    public let resolution: (width: Int, height: Int)?
    public let fps: Double?            // 仅视频有值
}

/// 格式支持级别
public enum FormatSupportLevel: Sendable {
    case fullDecode       // 完整解码（帧提取 + 音频 + 元数据）
    case metadataOnly     // 仅元数据（通过 sidecar 或容器头）
    case unsupported      // 完全不支持
}
```

## 2.4 MediaDecoder 协议

```swift
/// 媒体解码器协议
///
/// 每个解码器实现负责一类格式的解码能力。
/// CompositeMediaService 通过 probe() 评分选择最优解码器。
public protocol MediaDecoder: Sendable {
    /// 声明解码能力
    var capability: MediaCapability { get }

    /// 探测文件，返回支持评分和元数据
    func probe(filePath: String) async throws -> ProbeResult

    /// 提取关键帧图像
    func extractKeyframes(
        filePath: String,
        times: [Double],
        outputDir: String,
        maxDimension: Int
    ) async throws -> [String]

    /// 提取音频
    func extractAudio(
        filePath: String,
        outputPath: String,
        sampleRate: Int
    ) async throws -> String

    /// 获取视频帧用于 CLIP 编码（CGImage 直接输出）
    func extractFrames(
        filePath: String,
        times: [Double],
        targetSize: CGSize
    ) async throws -> [CGImage]
}

/// 场景检测能力（可选，不是所有解码器都支持）
public protocol SceneDetectable {
    func detectScenes(
        in filePath: String,
        threshold: Double
    ) async throws -> [SceneSegment]
}
```

## 2.5 MediaService 协议

```swift
/// 媒体服务协议 — 上层模块的唯一入口
///
/// 上层模块（LayeredIndexer、PipelineManager 等）
/// 仅依赖此协议，不直接依赖任何解码器实现。
public protocol MediaService: Sendable {
    func probe(filePath: String) async throws -> ProbeResult

    func detectScenes(
        filePath: String,
        config: SceneDetector.Config
    ) async throws -> [SceneSegment]

    func extractKeyframes(
        filePath: String,
        times: [Double],
        outputDir: String,
        maxDimension: Int
    ) async throws -> [String]

    func extractAudio(
        filePath: String,
        outputPath: String,
        sampleRate: Int
    ) async throws -> String

    func extractFrames(
        filePath: String,
        times: [Double],
        targetSize: CGSize
    ) async throws -> [CGImage]

    func getDuration(filePath: String) async throws -> Double

    func supportLevel(for filePath: String) async -> FormatSupportLevel
}
```

## 2.6 CompositeMediaService 路由引擎

```swift
/// 组合媒体服务 — GStreamer 风格的路由引擎
///
/// 管理所有注册的 MediaDecoder，为每个文件选择最优解码器。
/// 选择流程:
/// 1. 按扩展名过滤候选解码器
/// 2. 候选解码器执行 probe()，取 score 最高者
/// 3. 相同 score 取 priority 更高者
/// 4. 最高 score = 0 → 尝试通用 fallback（FFmpeg）
/// 5. 全部失败 → 尝试 metadata-only 解码器
/// 6. 完全不支持 → 抛出 MediaError.noDecoderAvailable
public final class CompositeMediaService: MediaService, @unchecked Sendable {

    private var decoders: [any MediaDecoder] = []
    private var decoderCache: [String: any MediaDecoder] = [:]

    /// 注册解码器（自动按 priority 降序排列）
    public func register(_ decoder: any MediaDecoder) {
        decoders.append(decoder)
        decoders.sort { $0.capability.priority > $1.capability.priority }
    }

    /// 为指定文件选择最优解码器
    private func bestDecoder(for filePath: String) async throws -> any MediaDecoder {
        // 1. 检查缓存
        // 2. 按扩展名过滤候选
        // 3. 检查可用性 + probe 评分
        // 4. fallback 到低优先级通用解码器
        // 5. 缓存结果并返回
    }

    // MediaService 协议方法——内部全部委托给选中的 decoder
}
```

## 2.7 具体解码器实现

### 2.7.1 AVFoundation 解码器 (Priority 80)

```swift
public final class AVFoundationDecoder: MediaDecoder, @unchecked Sendable {
    public let capability = MediaCapability(
        fileExtensions: ["mp4", "mov", "m4v"],
        utTypes: ["public.mpeg-4", "com.apple.quicktime-movie"],
        name: "AVFoundation",
        priority: 80
    )

    // probe: AVAsset.load(.isPlayable)，成功返回 score 90
    // extractKeyframes: AVAssetImageGenerator
    // extractAudio: AVAssetExportSession
    // extractFrames: AVAssetImageGenerator → CGImage (零拷贝可直传 CoreML)
}
```

**技术要点**：
- 对 H.264/H.265/ProRes 等 Apple 原生支持格式，比 FFmpeg 更快（硬件加速 VideoToolbox）
- `CVPixelBuffer` 直传 CoreML 模型，无需 JPEG 编解码中间步骤
- 对不支持的格式（MKV、DNxHR 等）probe 返回 score 0，自动 fallback 到 FFmpeg

### 2.7.2 FFmpeg 解码器 (Priority 50)

```swift
public final class FFmpegDecoder: MediaDecoder, @unchecked Sendable {
    public let capability = MediaCapability(
        fileExtensions: ["mp4", "mov", "avi", "mkv", "mxf", "ts", "flv", "webm"],
        utTypes: ["public.movie"],
        name: "FFmpeg",
        priority: 50
    )

    private let config: FFmpegConfig

    // 内部委托给现有 FFmpegBridge、KeyframeExtractor、AudioExtractor
    // FFmpegBridge.swift **不动**，FFmpegDecoder 是它的薄包装层
}
```

**关键设计**：FFmpegDecoder 不重写 FFmpeg 调用逻辑，而是复用现有 FFmpegBridge、SceneDetector、KeyframeExtractor、AudioExtractor 作为内部实现。

### 2.7.3 BRAW 解码器 (Priority 150)

```swift
public final class BRAWDecoder: MediaDecoder, @unchecked Sendable {
    public let capability = MediaCapability(
        fileExtensions: ["braw"],
        utTypes: [],
        name: "BlackmagicRAW",
        priority: 150
    )

    // isAvailable: 检查 BRAW SDK dylib 是否可加载
    // 通过 BRAW SDK C API 实现 probe / extractFrames / extractAudio
}
```

### 2.7.4 Sidecar Metadata 解码器 (Priority 10)

```swift
public struct SidecarMetadataDecoder: MediaDecoder {
    public let capability = MediaCapability(
        fileExtensions: ["r3d", "ari", "braw", "crm"],
        utTypes: [],
        name: "SidecarMetadata",
        priority: 10
    )

    // 即使无法解码视频，也能从 sidecar 文件提取元数据
    // probe: 返回 score 10，mediaType、duration 等
    // extractFrame: 抛出 MediaError.decodeNotSupported
}
```

### 2.7.5 优先级路由链

```
优先级路由链:

1. 原厂 SDK (P:150)
   ├── BRAW SDK → .braw 文件
   ├── RED SDK  → .r3d 文件（如果可获取）
   └── 其他 SDK → 按需添加

2. AVFoundation (P:80)
   ├── ProRes, H.264, H.265 → 原生硬件加速
   ├── R3D（如果用户装了 RED Apple Workflow Installer）
   └── 未来 macOS Media Extensions 自动生效

3. FFmpeg (P:50)
   ├── MKV, FLV, WMV 等冷门容器
   ├── MXF（AVFoundation 处理不了时）
   ├── DNxHD/DNxHR
   └── 通用 fallback

4. Sidecar Metadata (P:10)
   ├── 无法解码但可提取元数据
   └── 用于"仅索引"模式
```

## 2.8 格式检测机制

```swift
/// 基于文件头 magic bytes 的格式检测（比扩展名更可靠）
public struct FormatDetector {
    private static let signatures: [(bytes: [UInt8], offset: Int, format: String)] = [
        ([0x66, 0x74, 0x79, 0x70], 4, "mp4"),  // ftyp box
        ([0x1A, 0x45, 0xDF, 0xA3], 0, "mkv"),  // EBML header
        ([0x52, 0x49, 0x46, 0x46], 0, "avi"),  // RIFF
        ([0x06, 0x0E, 0x2B, 0x34], 0, "mxf"),  // SMPTE MXF
        ([0x52, 0x45, 0x44, 0x31], 0, "r3d"),  // RED1 magic
        ([0x52, 0x45, 0x44, 0x32], 0, "r3d"),  // RED2 magic
    ]

    public static func detect(fileURL: URL) throws -> String
    // 读取文件头 64 字节，匹配 magic bytes
    // Fallback: 使用文件扩展名
}
```

## 2.9 上游调用者改造策略

将 SceneDetector、KeyframeExtractor、AudioExtractor 中直接调用 FFmpeg 的代码改为通过 MediaService 协议调用。

**改造方式**：新增 `mediaService` 参数的方法重载，旧方法标记 `@available(*, deprecated)` 但保留不删除。所有现有测试继续使用旧方法，不受影响。

```swift
// SceneDetector 改造示例
public enum SceneDetector {
    // 现有方法 — 保留不变，标记 deprecated
    @available(*, deprecated, message: "Use detectScenes(filePath:config:mediaService:)")
    public static func detectScenes(
        filePath: String, config: Config, ffmpegConfig: FFmpegConfig
    ) async throws -> [SceneSegment] { /* 现有实现不变 */ }

    // 新方法 — 通过 MediaService 调用
    public static func detectScenes(
        filePath: String, config: Config, mediaService: any MediaService
    ) async throws -> [SceneSegment] { /* 委托给 mediaService */ }
}
```

---

# 第三部分：CLIP 嵌入与向量索引层

## 3.1 CLIP 编码器协议族

### CLIPImageEncoder

```swift
/// CLIP 图像编码器协议
public protocol CLIPImageEncoder: Sendable {
    var modelName: String { get }
    var dimensions: Int { get }
    var isLoaded: Bool { get }

    func load() async throws
    func encode(image: CGImage) async throws -> [Float]
    func encodeBatch(images: [CGImage]) async throws -> [[Float]]
}
```

### CLIPTextEncoder

```swift
/// CLIP 文本编码器协议
public protocol CLIPTextEncoder: Sendable {
    var modelName: String { get }
    var dimensions: Int { get }
    var isLoaded: Bool { get }

    func load() async throws
    func encode(text: String) async throws -> [Float]
    func encodeBatch(texts: [String]) async throws -> [[Float]]
}
```

### CLIPEmbeddingProvider

```swift
/// 整合图像编码器和文本编码器，提供统一的 CLIP 嵌入服务
///
/// 模型: google/siglip2-base-patch16-224 (768 维, 100+ 语言, Google 2025)
/// 状态: **核心搜索路径 (R2a)**，Spike 验证通过
/// 架构: 分离模型 + 投影权重 (vision 87ms, text 46ms)
public final class CLIPEmbeddingProvider: @unchecked Sendable {
    private let imageEncoder: any CLIPImageEncoder
    private let textEncoder: any CLIPTextEncoder

    /// 编码单帧图像
    public func encodeImage(_ image: CGImage) async throws -> [Float]

    /// 编码搜索查询（含 LRU 缓存）
    public func encodeQuery(_ text: String) async throws -> [Float]

    /// 编码图片查询（以图搜视频）
    public func encodeImageQuery(_ image: CGImage) async throws -> [Float]
}
```

**模型选择说明**（详见 [02\_技术决策记录 §2.3](./02_技术决策记录.md)）：

| 模型 | 维度 | 优势 | 劣势 | 状态 |
|------|------|------|------|------|
| **SigLIP2-base** (google/siglip2-base-patch16-224) | 768 | 100+ 语言, XM3600 40.7%, Spike 验证通过 | 分离模型 ~715MB, tokenizer 需 -1 修正 | **R2a 核心: CLIP 文字搜视频** |
| **EmbeddingGemma-300M** (ONNX Q8) | 768 | MTEB <500M 第一, 100+ 语言, 完全离线 | ONNX Runtime 依赖 | **R2c: L3 本地 text embedding** |
| Gemini gemini-embedding-001 | 768 | 质量高, 与 EmbeddingGemma 同维度 | 需网络 + API Key | **R2c: L3 在线 text embedding** |

**架构说明**: 核心搜索路径是 CLIP 向量 (L1, R2a) + FTS5 (L2)。L3 文本描述 + text embedding (EmbeddingGemma/Gemini) 作为增强层，弥补 CLIP 在属性绑定等弱势场景的不足，也是付费用户差异化来源。

## 3.2 VectorIndexEngine 协议

```swift
/// 向量索引引擎协议
public protocol VectorIndexEngine: Sendable {
    /// 添加向量
    func add(key: Int64, vector: [Float]) throws
    /// 批量添加
    func addBatch(keys: [Int64], vectors: [[Float]]) throws
    /// 删除向量
    func remove(key: Int64) throws
    /// 搜索最近邻
    func search(query: [Float], count: Int) throws -> [(key: Int64, distance: Float)]
    /// 向量总数
    var count: Int { get }
    /// 持久化到磁盘
    func save(to path: String) throws
    /// 从磁盘加载
    static func load(from path: String, mmap: Bool) throws -> Self
}
```

## 3.3 USearch HNSW 实现

```swift
/// USearch HNSW 向量索引
///
/// 选型理由详见 02_技术决策记录 ADR-015
/// 120 万 clips 768 维 FP16 → 搜索 < 1ms，内存 ~1.1GB
public final class USearchVectorIndex: VectorIndexEngine, @unchecked Sendable {
    private let index: USearchIndex

    // HNSW 配置参数
    static let dimensions: UInt32 = 768
    static let connectivity: UInt32 = 16        // M parameter
    static let efConstruction: UInt32 = 200
    static let efSearch: UInt32 = 64
    static let quantization: USearchScalar = .F16

    // 索引文件路径 (双索引)
    static let clipIndexPath = "~/Library/Application Support/FindIt/clip.usearch"
    static let textIndexPath = "~/Library/Application Support/FindIt/text.usearch"
}
```

### 与现有架构的集成

**双索引架构**: CLIP 向量和文本嵌入向量在不同嵌入空间，不能混合搜索。

```
SQLite BLOB（source of truth，向量持久化）
    ├── clip_vectors 表 (CLIP space, L1 产出)
    └── clips.embedding 列 (text embedding space, L3 产出)
    ↓ App 启动时 / 索引完成时
USearch 索引 A: clip.usearch (CLIP 向量, L1)
    → 搜索: query → SigLIP2 text encoder → 搜索 A
USearch 索引 B: text.usearch (文本嵌入, L3)
    → 搜索: query → EmbeddingGemma/Gemini → 搜索 B
    ↓ 搜索时分别调用
SearchEngine 三路融合: FTS5 + 索引 A (CLIP) + 索引 B (text) → 加权排序
```

- SQLite BLOB 保持为 source of truth（双层存储理念不变）
- 两个 USearch 索引均视为缓存，可从 SQLite 完整重建
- 新 clip 索引完成时同时写 SQLite BLOB + `USearch.add()`
- App 启动时 mmap 加载 .usearch 文件，如不存在则从 SQLite 重建

### 索引生命周期

```
App 启动
    ├── vector.usearch 存在？
    │   ├── YES → mmap 加载 (≈ 0ms)
    │   └── NO  → 从 SQLite BLOB 全量重建 (120 万 clips ≈ 2-3 分钟)
    │
索引新 clip
    ├── 1. CLIP 编码关键帧 → embedding [Float]
    ├── 2. 写入 SQLite clip_vectors.vector BLOB (source of truth)
    ├── 3. USearch index.add(clipId, embedding)
    └── 4. 每 1000 条自动 save() 到磁盘

删除 clip
    ├── 1. 从 SQLite 删除 clip 记录
    └── 2. USearch index.remove(clipId)

搜索
    ├── CLIP 路径: text → SigLIP2 text encoder → USearch-A (CLIP 索引)
    ├── Text embedding 路径: text → EmbeddingGemma/Gemini → USearch-B (text 索引)
    ├── FTS5 路径: text → FTS5 关键词搜索
    ├── 图片查询: image → SigLIP2 image encoder → USearch-A (纯 CLIP)
    └── 三路结果 → 自适应加权融合 → Top-K
```

### 内存策略

| 设备内存 | 加载策略 | 搜索延迟 |
|---------|---------|---------|
| 16GB+ | 全量加载到内存 | < 1ms |
| 8GB | mmap 模式 (OS 按需页入) | < 5ms |

代码路径完全一致，仅 `USearchIndex.load()` 参数不同。

## 3.4 LRU 嵌入缓存 (ADR-018)

```swift
/// 文本查询嵌入 LRU 缓存
/// 容量 256 条，相同查询不重复调用模型推理
/// 参考 memvid 的 1000 条 LRU 缓存设计
actor EmbeddingCache {
    private var cache: OrderedDictionary<String, [Float]> = [:]
    private let capacity: Int = 256

    func get(_ text: String) -> [Float]?
    func set(_ text: String, embedding: [Float])
}
```

---

# 第四部分：分层索引管线

## 4.1 LayeredIndexer 设计

取代 PipelineManager 的 6 阶段顺序管线，改为 4 层按需索引，每层可独立运行、独立跳过。

```swift
/// 分层索引策略
///
/// Layer 0: 元数据提取 (< 1s/文件)
///   → MediaService.probe() 获取时长、分辨率、编码格式、媒体类型
///   → 写入 videos 表（含 media_type），状态变为 metadata_done
///   → 用户立即可见条目（虽然尚未可搜索）
///
/// Layer 1: CLIP 向量索引 (10-60s/视频)
///   → 视频: 场景检测 → 每段采样帧 → CLIP 编码
///   → 照片: 直接加载图像 → CLIP 单帧编码 (< 50ms/张)
///   → 音频: ★ 跳过（无视觉内容）
///   → 写入 clip_vectors 表 + 同步到 USearch → 可搜索!
///
/// Layer 2: 语音转录 (1-5min)
///   → 视频: 提取音频 → STTProcessor 转录
///   → 照片: ★ 跳过（无音频）
///   → 音频: 直接 → STTProcessor 转录
///   → 写入 clips.transcript + clips_fts → 台词搜索可用
///
/// Layer 3: 文本描述（可选，后台空闲时）
///   → 视频/照片: LocalVisionAnalyzer / LocalVLMAnalyzer 描述
///   → 音频: ★ 跳过（无视觉内容）
///   → 写入 clips.description 等字段 → FTS5 关键词搜索增强
///
/// 关键设计: Layer 1 完成后即可搜索（不必等 Layer 2/3）
public enum LayeredIndexer {

    public enum Layer: Int, CaseIterable, Comparable {
        case metadata = 0
        case clipVector = 1
        case stt = 2
        case textDescription = 3
    }
}
```

## 4.2 多媒体类型适用性矩阵

| Layer | 视频 | 照片 | 音频 |
|-------|------|------|------|
| L0 元数据 | ✓ | ✓ | ✓ |
| L1 CLIP 向量 | ✓ | ✓ | 跳过 |
| L2 语音转录 | ✓ | 跳过 | ✓ |
| L3 文本描述 | ✓ | ✓ | 跳过 |

```swift
/// 判断指定层是否适用于给定媒体类型
public func isApplicable(for mediaType: MediaType) -> Bool {
    switch (self, mediaType) {
    case (.metadata, _):            return true
    case (.clipVector, .video):     return true
    case (.clipVector, .photo):     return true
    case (.clipVector, .audio):     return false
    case (.stt, .video):            return true
    case (.stt, .photo):            return false
    case (.stt, .audio):            return true
    case (.textDescription, .video):  return true
    case (.textDescription, .photo):  return true
    case (.textDescription, .audio):  return false
    }
}
```

## 4.3 Layer 1 CLIP 向量索引细节

**视频文件处理流程**:
1. `MediaService.detectScenes()` → 获取场景分割时间点
2. 每个场景 → `MediaService.extractFrames()` 采样关键帧
3. 关键帧 → `CLIPImageEncoder.encode()` / `encodeBatch()` → 向量
4. 向量 → 写入 `clip_vectors` 表 + `USearch.add()`
5. 状态变为 `vectors_done` → **用户可搜索**

**照片文件处理流程**（架构预留）:
1. 直接加载 CGImage（无需场景检测、无需提取帧）
2. `CLIPImageEncoder.encode(image)` → 向量 (< 50ms)
3. 写入 `clip_vectors` 表 → 即刻可搜索

## 4.4 索引状态机

```
旧状态流转:
pending → stt_running → stt_done → vision_running → completed / failed

新状态流转:
pending → metadata_done → vectors_done → stt_done → completed / failed / orphaned

Layer 与状态的对应:
  Layer 0 完成 → metadata_done (index_layer = 0)
  Layer 1 完成 → vectors_done  (index_layer = 1)  ← 搜索可用分界线
  Layer 2 完成 → stt_done      (index_layer = 2)
  Layer 3 完成 → completed     (index_layer = 3)
```

Migration 时旧状态映射：
- `completed` → `index_layer = 3`
- `stt_done` → `index_layer = 2`
- 其他 → `index_layer = 0`

## 4.5 Vision 分析器三级策略

Layer 3 的视觉描述分析使用三级回退策略：

| 优先级 | 分析器 | 字段覆盖 | 延迟 | 网络 |
|--------|--------|---------|------|------|
| 1 | Gemini 2.5 Flash (远程) | 9/9 字段 | 2-5s | 需 API Key |
| 2 | LocalVLMAnalyzer (本地 VLM) | 9/9 字段 | 1-3s | 无 |
| 3 | LocalVisionAnalyzer (Apple Vision) | 6/9 字段 | 10-30ms | 无 |

---

# 第五部分：查询管线与搜索引擎

## 5.1 QueryPipeline 查询预处理

```swift
/// 查询预处理管线
///
/// 处理流程:
/// 1. 语言检测（CJK? 英文? 混合?）
/// 2. 翻译（如 SigLIP 多语言能力不足，中→英翻译作为增强）
/// 3. 同义词扩展（"海滩" → "beach", "seashore"）
/// 4. 负向查询解析（"-人物" → 排除条件）
/// 5. 多查询向量生成
public enum QueryPipeline {

    public struct ProcessedQuery: Sendable {
        public let original: String
        public let language: DetectedLanguage
        public let englishTranslation: String?
        public let expandedQueries: [String]
        public let ftsExpression: String
    }

    public enum DetectedLanguage {
        case english
        case chinese
        case mixed
        case other(String)
    }

    public static func process(
        query: String,
        translator: TranslationBridge?
    ) async throws -> ProcessedQuery
}
```

**注意**：核心搜索走 SigLIP2 CLIP (100+ 语言原生支持)，补充搜索走 EmbeddingGemma/Gemini (同样多语言)。翻译层保留为可选增强，主要用于：(1) 同义词扩展提升 FTS5 召回率；(2) 极端语言 (如小众语种) 回退到英文翻译。SigLIP2 中文能力 Spike 已验证 (ZH/EN > 60%)。

## 5.2 TranslationBridge 翻译协议

```swift
/// 翻译桥接协议
///
/// 实现方案优先级:
/// 1. Apple Translation framework (macOS 15+, 离线, 免费)
/// 2. NLLanguageRecognizer + 本地词典映射 (macOS 14 兼容)
/// 3. 未来: Foundation Models (macOS 26+)
public protocol TranslationBridge: Sendable {
    func detectLanguage(_ text: String) -> QueryPipeline.DetectedLanguage
    func translateToEnglish(_ text: String) async throws -> String
    func expandSynonyms(
        _ text: String,
        language: QueryPipeline.DetectedLanguage
    ) -> [String]
}
```

## 5.3 搜索查询类型

```swift
/// 搜索查询枚举
public enum SearchQuery {
    case text(String)                    // 纯文本
    case image(Data)                     // 以图搜视频 (ADR-016)
    case textWithImage(String, Data)     // 文本 + 参考图片
}

/// 解析后的查询
public struct ParsedQuery {
    let positiveQuery: String
    let negativeQuery: String?           // "-人物" → "人物" (ADR-017)
    let isExactMatch: Bool               // 引号精确匹配
    let hasImage: Bool
}
```

## 5.4 SearchEngine 修订设计

```swift
public enum SearchEngine {

    /// 搜索（支持文本和图片查询）
    public static func search(
        query: SearchQuery,
        db: DatabasePool,
        clipIndex: VectorIndexEngine,       // CLIP 向量索引 (L1)
        textIndex: VectorIndexEngine?,      // 文本嵌入索引 (L3, 可选)
        clipEncoder: CLIPEmbeddingProvider,
        textEmbedder: (any EmbeddingProvider)?,  // L3 text embedding
        mode: SearchMode = .auto,
        limit: Int = 50
    ) async throws -> [SearchResult] {

        // 1. 解析查询（提取正向/负向关键词）
        let parsed = QueryParser.parse(query)

        // 2. CLIP 向量搜索 (USearch-A, < 1ms)
        //    query → SigLIP2 text encoder → CLIP space 查询向量
        let clipQueryEmb = try await clipEncoder.encodeQuery(parsed.positiveQuery)
        var clipResults = try clipIndex.search(query: clipQueryEmb, count: limit * 3)

        // 3. 负向查询扣分 (ADR-017)
        if let negQuery = parsed.negativeQuery {
            let negEmb = try await clipEncoder.encodeQuery(negQuery)
            let negResults = try clipIndex.search(query: negEmb, count: limit * 3)
            clipResults = applyNegativeScoring(clipResults, negResults, lambda: 0.5)
        }

        // 4. FTS5 关键词搜索
        let ftsResults = try await ftsSearch(parsed, db: db, limit: limit * 3)

        // 5. 文本嵌入搜索 (USearch-B, 仅 L3 数据存在时)
        //    query → EmbeddingGemma/Gemini → text embedding space 查询向量
        var textResults: [(key: Int64, distance: Float)] = []
        if let textIndex, let textEmbedder {
            let textQueryEmb = try await textEmbedder.embed(parsed.positiveQuery)
            textResults = try textIndex.search(query: textQueryEmb, count: limit * 3)
        }

        // 6. 自适应融合排序（权重根据可用数据层动态调整）
        let weights = determineWeights(for: parsed, hasTextIndex: !textResults.isEmpty)
        return fuseResults(clip: clipResults, fts: ftsResults, text: textResults,
                          weights: weights, limit: limit)
    }
}
```

## 5.5 混合搜索融合权重

重构后搜索有三条路径可用：

| 路径 | 数据来源 | 搜索能力 |
|------|---------|---------|
| CLIP 向量 (USearch) | clip_vectors 表 (Layer 1) | 视觉语义匹配 |
| FTS5 关键词 | clips_fts 表 (Layer 2/3) | 精确关键词、台词 |
| 文本嵌入向量 (VectorStore) | clips.embedding (Layer 3) | 描述语义匹配（兼容现有数据） |

融合权重（自适应策略）：

根据**可用数据层**和**查询类型**动态调整：

| 场景 | CLIP 向量 | FTS5 | 文本嵌入 | 触发条件 |
|------|----------|------|---------|---------|
| L1 完成 (仅 CLIP) | **0.7** | 0.3 | 0.0 | textIndex 为空 |
| L1+L2 完成 | **0.6** | 0.4 | 0.0 | 有 STT 转录但无 L3 |
| L1+L2+L3 完成 | **0.5** | 0.2 | 0.3 | 三路均可用 |
| 引号精确匹配 | 0.1 | **0.8** | 0.1 | `isExactMatch` |
| 长句 (>10 字) | **0.6** | 0.1 | 0.3 | 查询长度 |
| 图片查询 | **1.0** | 0.0 | 0.0 | `SearchQuery.image` |

---

# 第六部分：数据库 Schema 变更

## 6.1 文件夹级库变更

### 新增表：clip_vectors

```sql
-- CLIP 向量表（独立于 clips 表，因为维度不同于文本嵌入）
CREATE TABLE clip_vectors (
    vector_id       INTEGER PRIMARY KEY AUTOINCREMENT,
    clip_id         INTEGER NOT NULL REFERENCES clips(clip_id),
    model_name      TEXT NOT NULL,           -- "siglip2-base" / "chinese-clip"
    dimensions      INTEGER NOT NULL,        -- 768 / 512
    vector          BLOB NOT NULL,           -- Float32 数组
    created_at      TEXT NOT NULL DEFAULT (datetime('now')),
    UNIQUE(clip_id, model_name)              -- 一个 clip 可有多个模型的向量
);
```

**clip_vectors 独立于 clips.embedding 的理由**：
1. CLIP 向量 (768 维) 与文本嵌入向量 (768 维 Gemini / 512 维 NL) 维度可能不同
2. 一个 clip 可以同时有 CLIP 向量和文本嵌入向量
3. 未来升级模型时，可以为同一 clip 存储多个模型的向量
4. 不破坏现有 `clips.embedding` 的数据和逻辑

### videos 表新增字段

```sql
ALTER TABLE videos ADD COLUMN media_type TEXT DEFAULT 'video';
    -- ★ 预埋: 'video' | 'photo' | 'audio'
ALTER TABLE videos ADD COLUMN container_format TEXT;
    -- "mov", "mp4", "mxf", "r3d", "braw", "jpg", "wav"
ALTER TABLE videos ADD COLUMN codec TEXT;
    -- "h264", "prores", "redcode", "braw", "jpeg", "aac"
ALTER TABLE videos ADD COLUMN resolution_width INTEGER;
ALTER TABLE videos ADD COLUMN resolution_height INTEGER;
ALTER TABLE videos ADD COLUMN fps REAL;
    -- 仅视频有值，照片/音频为 NULL
ALTER TABLE videos ADD COLUMN decoder_used TEXT;
    -- "avfoundation" / "ffmpeg" / "braw-sdk"
ALTER TABLE videos ADD COLUMN index_layer INTEGER DEFAULT 0;
    -- 当前已完成到哪一层 (0-3)
```

### index_status 扩展

```
旧值: pending / stt_running / stt_done / vision_running / completed / failed / orphaned
新值: pending / metadata_done / vectors_done / stt_done / completed / failed / orphaned
```

### clips 表语义扩展

`start_time` / `end_time` 按 media_type 有不同语义：
- **视频**: 场景片段时间范围（现有语义不变）
- **照片**: start_time = 0, end_time = 0（整张照片）
- **音频**: 按静默点或固定间隔切分的音频片段

列约束 NOT NULL 保留，照片统一写 0。

## 6.2 全局搜索索引变更

### 新增表：clip_vectors

```sql
-- 全局 clip_vectors 表（同步自文件夹库）
CREATE TABLE clip_vectors (
    vector_id        INTEGER PRIMARY KEY AUTOINCREMENT,
    clip_id          INTEGER NOT NULL REFERENCES clips(clip_id),
    source_folder    TEXT NOT NULL,
    source_vector_id INTEGER NOT NULL,
    model_name       TEXT NOT NULL,
    dimensions       INTEGER NOT NULL,
    vector           BLOB NOT NULL,
    UNIQUE(source_folder, source_vector_id)
);
```

### 可选新增表：query_cache

```sql
-- 查询缓存表（避免重复编码相同查询）
CREATE TABLE query_cache (
    query_text      TEXT PRIMARY KEY,
    query_vector    BLOB NOT NULL,
    model_name      TEXT NOT NULL,
    created_at      TEXT NOT NULL DEFAULT (datetime('now'))
);
```

## 6.3 Migration 策略

在 `Migrations.swift` 中新增版本化迁移：

```swift
// Migration v6: 分层索引支持
migrator.registerMigration("v6_layered_indexing") { db in
    // 1. 创建 clip_vectors 表
    try db.create(table: "clip_vectors") { t in
        t.autoIncrementedPrimaryKey("vector_id")
        t.column("clip_id", .integer).notNull().references("clips")
        t.column("model_name", .text).notNull()
        t.column("dimensions", .integer).notNull()
        t.column("vector", .blob).notNull()
        t.column("created_at", .text).notNull().defaults(sql: "datetime('now')")
        t.uniqueKey(["clip_id", "model_name"])
    }

    // 2. videos 表新增字段
    try db.alter(table: "videos") { t in
        t.add(column: "media_type", .text).defaults(to: "video")
        t.add(column: "container_format", .text)
        t.add(column: "codec", .text)
        t.add(column: "resolution_width", .integer)
        t.add(column: "resolution_height", .integer)
        t.add(column: "fps", .double)
        t.add(column: "decoder_used", .text)
        t.add(column: "index_layer", .integer).defaults(to: 0)
    }

    // 3. 状态迁移: 将已有 completed 视频标记为 index_layer = 3
    try db.execute(sql: """
        UPDATE videos SET index_layer = 3 WHERE index_status = 'completed'
    """)
    try db.execute(sql: """
        UPDATE videos SET index_layer = 2 WHERE index_status = 'stt_done'
    """)
}
```

---

# 第七部分：数据流

## 7.1 视频文件索引流程

```
视频文件 (.mp4, .mov, .mkv, .braw, ...)
    │
    ├─ Layer 0: 元数据 (< 1s)
    │   CompositeMediaService.probe(filePath)
    │   → media_type='video', duration, resolution, codec, fps, decoder_used
    │   → INSERT INTO videos (状态: metadata_done, index_layer: 0)
    │   → 用户可见条目
    │
    ├─ Layer 1: CLIP 向量 (10-60s)
    │   ├─ MediaService.detectScenes() → SceneSegment[]
    │   ├─ 每个 segment → MediaService.extractFrames(采样帧)
    │   ├─ CLIPImageEncoder.encode/encodeBatch() → 向量
    │   └─ INSERT INTO clip_vectors → USearch.add()
    │   → 状态: vectors_done, index_layer: 1 → 可搜索!
    │
    ├─ Layer 2: 语音转录 (1-5min)
    │   ├─ MediaService.extractAudio() → 16kHz WAV
    │   ├─ STTProcessor.transcribe() → 带时间戳文本
    │   └─ UPDATE clips SET transcript = ...
    │   → 状态: stt_done, index_layer: 2 → 台词搜索可用
    │
    └─ Layer 3: 文本描述 (可选, 后台)
        ├─ 分析器选择: Gemini > LocalVLM > LocalVisionAnalyzer
        ├─ 生成 scene/subjects/actions/... 描述
        └─ UPDATE clips SET description/tags = ...
        → 状态: completed, index_layer: 3 → FTS 增强
```

## 7.2 照片文件索引流程（架构预留）

```
照片文件 (.jpg, .heic, .png, .raw, .dng, ...)
    │
    ├─ Layer 0: 元数据 (< 0.1s)
    │   MediaService.probe(filePath)
    │   → media_type='photo', resolution, 无 duration/fps
    │   → INSERT INTO videos (media_type='photo')
    │   → 创建 1 个 clip (start_time=0, end_time=0)
    │
    ├─ Layer 1: CLIP 向量 (< 50ms)
    │   ├─ 直接加载 CGImage（无需场景检测）
    │   ├─ CLIPImageEncoder.encode(image) → 向量
    │   └─ INSERT INTO clip_vectors → 即刻可搜索
    │
    ├─ Layer 2: ★ 跳过（照片无音频）
    │
    └─ Layer 3: 文本描述 (可选)
        └─ LocalVisionAnalyzer / LocalVLMAnalyzer 描述照片
```

## 7.3 音频文件索引流程（架构预留）

```
音频文件 (.mp3, .wav, .aac, .flac, .m4a, ...)
    │
    ├─ Layer 0: 元数据 (< 0.1s)
    │   MediaService.probe(filePath)
    │   → media_type='audio', duration, 无 resolution/fps
    │
    ├─ Layer 1: ★ 跳过（音频无视觉内容）
    │
    ├─ Layer 2: 语音转录 (1-5min)
    │   ├─ 文件本身即音频，无需 extractAudio()
    │   ├─ STTProcessor.transcribe() → 带时间戳文本
    │   └─ 按静默点或固定间隔创建多个 clips
    │
    └─ Layer 3: ★ 跳过（无视觉内容）
```

## 7.4 搜索数据流

```
用户查询 "海滩日落"
    │
    ├─ QueryPipeline.process()
    │   ├─ 语言检测 → chinese
    │   ├─ 翻译 (如需) → "beach sunset"
    │   ├─ 扩展 → ["beach sunset", "ocean sunset", "seaside dusk"]
    │   └─ FTS 表达式 → "海滩 OR 日落 OR beach OR sunset"
    │
    ├─ CLIP 向量搜索 (USearch-A, CLIP space, < 1ms) ← 核心路径
    │   ├─ SigLIP2 text encoder.encode("beach sunset") → 768 维 CLIP 查询向量
    │   ├─ USearch-A.search(queryVector, topK: 150) → 候选集 A
    │   └─ 扩展查询向量融合得分
    │
    ├─ FTS5 搜索 (关键词 + 转录, L2 产出)
    │   ├─ FTS5.match("海滩 OR 日落 OR beach OR sunset") → 候选集 B
    │   └─ 基于 BM25 排序
    │
    ├─ 文本嵌入搜索 (USearch-B, text embedding space, L3 产出, 可选)
    │   ├─ EmbeddingGemma/Gemini.embed("海滩日落") → 768 维 text 查询向量
    │   ├─ USearch-B.search(queryVector, topK: 150) → 候选集 C
    │   └─ 仅 L3 完成的 clips 有结果
    │
    └─ 自适应融合
        ├─ 权重根据可用数据层动态调整 (见 §5.5)
        ├─ Min-Max 归一化 → 加权求和 → 排序
        └─ Top-K → SearchResult[]
```

## 7.5 以图搜视频数据流

```
用户拖入参考图片
    │
    ├─ CLIPImageEncoder.encode(image) → 768 维查询向量
    │
    ├─ USearch.search(queryVector, topK: 50) → 视觉相似片段
    │
    └─ 直接返回（不经过 FTS5，纯向量搜索）
```

---

# 第八部分：性能指标与基准

## 8.1 搜索延迟目标

| 规模 | clips 数 | 暴力扫描 (现有) | USearch HNSW (目标) | 达标? |
|------|---------|---------------|---------------------|-------|
| 个人 | 10,000 | 2.5ms | **0.1ms** | ✅ < 5ms |
| 中度 | 100,000 | 25ms | **0.3ms** | ✅ < 5ms |
| 重度 | 500,000 | 125ms | **0.5ms** | ✅ < 5ms |
| 团队 | 1,000,000 | 250ms ❌ | **0.6ms** | ✅ < 5ms |
| 极限 (20TB) | 1,200,000 | 307ms ❌ | **< 1ms** | ✅ < 5ms |

## 8.2 内存占用目标

| 规模 | clips 数 | FP32 (现有) | FP16 HNSW (目标) |
|------|---------|-----------|------------------|
| 10TB 中等 | 200,000 | 0.6 GB | **0.3 GB** |
| 20TB 极端 | 1,200,000 | 3.5 GB | **1.8 GB** (全量) / **按需** (mmap) |

## 8.3 索引速度目标

| 阶段 | 每 clip 耗时 | 100K clips | 1M clips |
|------|------------|-----------|---------|
| CLIP 编码 (SigLIP) | ~30-50ms | ~50 分钟 | ~8 小时 |
| USearch 插入 | ~0.1ms | ~10 秒 | ~2 分钟 |
| FTS5 写入 | ~0.05ms | ~5 秒 | ~50 秒 |
| **瓶颈**: CLIP 编码 | — | — | 可并行，CoreML ANE 加速 |

## 8.4 数据规模换算

设计目标：支持 10TB-20TB 视频素材的完整索引和毫秒级搜索。

| 素材总量 | 视频均大小 | 视频数 | clips/视频 | 总 clip 数 | FP16 内存 |
|---------|----------|--------|-----------|------------|----------|
| 10TB | 500MB | 20,480 | 20 | 409,600 | 0.6 GB |
| 10TB | 1GB | 10,240 | 20 | 204,800 | 0.3 GB |
| 20TB | 500MB | 40,960 | 20 | 819,200 | 1.2 GB |
| 20TB | 500MB | 40,960 | 30 | 1,228,800 | 1.8 GB |

## 8.5 USearch HNSW 20TB 极端场景预估

120 万 clips，768 维，FP16，M=16，efConstruction=200，efSearch=64：

| 指标 | 数值 |
|------|------|
| 搜索延迟 | **< 1ms** |
| 内存占用 | ~1.1 GB (mmap 可按需加载) |
| 索引文件大小 | ~1.0 GB |
| 索引构建时间 | ~2-3 分钟 (一次性) |
| Recall@10 | ~93-94% |
| App 启动加载 | mmap 模式 **≈ 0ms** |

## 8.6 重构前后对比总结

| 维度 | 重构前 (现有) | 重构后 (目标) |
|------|-------------|-------------|
| **索引单帧** | 2-5s (Gemini API) / 1-3s (本地 VLM) | **~30-50ms** (CLIP CoreML) |
| **每分钟视频索引** | 2-4 min (含 STT/Vision) | **~10-30s** (CLIP + 场景检测) |
| **搜索延迟 (100K)** | 50-200ms (FTS5 + Accelerate) | **< 5ms** (USearch HNSW) |
| **搜索延迟 (1M)** | 250ms+ (暴力扫描瓶颈) | **< 1ms** (USearch HNSW) |
| **内存 (索引时)** | 3GB (VLM) + 500MB (WhisperKit) | **~80-100MB** (CLIP 模型) + 按需 WhisperKit |
| **API 依赖** | Gemini Flash (必需或可选) | **无** (完全本地) |
| **格式覆盖** | H.264/H.265/ProRes + FFmpeg 支持格式 | + BRAW + 未来 R3D/ARRIRAW |
| **中文搜索** | 仅 FTS5 文本匹配 | **CLIP 多语言视觉匹配** + FTS5 |
| **以图搜视频** | 不支持 | **支持** (CLIP 跨模态) |
| **负向查询** | 不支持 | **支持** (向量减法 + FTS NOT) |

---

## 附录 A：新增文件清单

| 文件 | 用途 | Phase |
|------|------|-------|
| `Sources/FindItCore/Media/MediaDecoder.swift` | 解码器协议 + 核心类型定义 | R1 |
| `Sources/FindItCore/Media/MediaService.swift` | 媒体服务协议 | R1 |
| `Sources/FindItCore/Media/CompositeMediaService.swift` | 路由引擎 | R1 |
| `Sources/FindItCore/Media/AVFoundationDecoder.swift` | AVFoundation 解码器 | R1 |
| `Sources/FindItCore/Media/FFmpegDecoder.swift` | FFmpeg 解码器（封装现有 FFmpegBridge） | R1 |
| `Sources/FindItCore/Media/FormatDetector.swift` | magic bytes 格式检测 | R1 |
| `Sources/FindItCore/Search/CLIPImageEncoder.swift` | CLIP 图像编码器协议 + SigLIP2 实现 | R2a |
| `Sources/FindItCore/Search/CLIPTextEncoder.swift` | CLIP 文本编码器协议 + SigLIP2 实现 | R2a |
| `Sources/FindItCore/Search/CLIPEmbeddingProvider.swift` | CLIP 统一嵌入服务 + LRU 缓存 | R2a |
| `Sources/FindItCore/Search/EmbeddingGemmaProvider.swift` | 本地 text embedding (L3 离线) | R2c |
| `Sources/FindItCore/Pipeline/LayeredIndexer.swift` | 分层索引调度器 | R3 |
| `Sources/FindItCore/Search/QueryPipeline.swift` | 查询预处理管线 | R4 |
| `Sources/FindItCore/Search/TranslationBridge.swift` | 翻译桥接协议 | R4 |
| `Sources/FindItCore/Media/BRAWDecoder.swift` | Blackmagic RAW SDK 解码器 | R5 |

## 附录 B：需修改文件清单

| 文件 | 改动类型 | Phase |
|------|---------|-------|
| `PipelineManager.swift` | 重构核心逻辑，从 6 阶段改为调用 LayeredIndexer | R3 |
| `SceneDetector.swift` | 新增 mediaService 参数重载，旧方法标记 deprecated | R1 |
| `KeyframeExtractor.swift` | 新增 mediaService 参数重载 | R1 |
| `AudioExtractor.swift` | 新增 mediaService 参数重载 | R1 |
| `FFmpegBridge.swift` | 无直接改动，降级为 FFmpegDecoder 内部实现 | R1 |
| `SearchEngine.swift` | 新增 CLIP 向量搜索路径、以图搜视频、负向查询 | R2c |
| `VectorStore.swift` | 支持多维度向量共存 + CLIP 向量独立存储 | R2b |
| `Migrations.swift` | 新增 v6\_layered\_indexing migration | R2 |
| `FileScanner.swift` | 新增 MediaType 枚举、mediaType(for:) 分类方法 | R3 |
| `IndexingScheduler.swift` | 适配新管线签名 | R3 |
| `IndexingManager.swift` | 创建 CompositeMediaService 实例、注入依赖 | R1/R3 |
| `SearchState.swift` | 增加 CLIP 搜索路径 | R4 |
| `Package.swift` | 新增 USearch 依赖、移除可选依赖 | R2b |

## 附录 C：Package.swift 修订

```swift
// 新增依赖
.package(url: "https://github.com/unum-cloud/usearch", from: "2.0.0"),

// FindItCore target 新增
.product(name: "USearch", package: "usearch"),
```

---

> 本文档定义了重构后的完整架构规格。所有协议定义、数据流、Schema 变更和性能指标构成了重构执行的技术约束。具体的分期任务拆分、文件级改动步骤和验收标准，请参见 [04\_重构执行路线图](./04_重构执行路线图.md)。
